# <p style="color: #2563EB">Spring Data JPA Best Practices</p>

## Table of contents

1. [Associations](#Associations)
   1. [How to effectively shape the @OneToMany Associations](#how-to-effectively-shape-the-onetomany-associations)
   2. [Why should you avoid the unidirectional @OneToMany Association](#why-should-you-avoid-the-unidirectional-onetomany-association)

# Associations

## How to effectively shape the @OneToMany Associations

consider two entities Author and Book involved in a bidirectional lazy @OneToMany association

![image](images/imgone.png)

So the Author table has @OneToMany relationship with the book table. An author row can be referenced by multiple book rows.

The author_id column maps this relationship via foreign key that references the primary key of the author table.

A book column cannot exist without an author therefore the author is the parent-side (@OneToMany) while the book is the

child side (@ManyToOne)

The @ManyToOne association is responsible for synchronizing the foreign key column with the persistence context (the first level cache)

Rule of thumb

Always use bidirectional @OneToMany associations instead of unidirectional ones due to performance constraints as explained in the section below

### Best way to code a bidirectional @OneToMany association

1. Always cascade from parent side to child side

   Cascading from child side to parent-side is a code smell and a bad practice and it is a clear signal that its time to review your domain model

   Think of how illogical it is for a child to cascade the creation of a parent

   in this case we cascade from Authors side to the Book side so we add the cascade type in the Author entity

   `@OneToMany(cascade = CascadeType.All)`

   Never use CascadeType.* on @ManyToOne since entity state transitions should be propagated from parent-side entities to child-side entities
2. Don't forget to set mappedby on the parent side

   The mappedBy attribute characterizes a bidirectional association and must be set on the parent side

   in other words for a bidirectional @OneToMany association, set mappedBy to @OneToMany on the parent side and add @ManyToOne on the child-side

   referenced by mappedBy

   Via mappedBy, the bidirectional @OneToMany association signals that it mirrors the @ManyToOne child-side mapping

   in this case we add

   `@OneToMany(cascade = CascadeType.ALL, mappedBy = "author")`
3. set orphanRemoval on the parent side

   Setting orphanRemoval on the parent-side guarantees the removal of children without references.

   Example

   `@OneToMany(cascade = CascadeType.ALL, mappedBy = "author", orphanRemoval = true)`
4. Keep both sides of the Association in sync

   You can easily keep the both sides of the association in sync via helper methods added on the parent side

   commonly the addChild(), removeChild() and removeChildren()

   if you do not strive to keep both sides of the association in sync, the entity state transitions may lead to unexpected behaviours

   in this case we add the following helpers

   ```
   public void addBook(Book book) {
   this.books.add(book);
   book.setAuthor(this);
   }

   public void removeBook(Book book) {
   book.setAuthor(null);
   this.books.remove(book);
   }

   public void removeBooks() {
   Iteratorbook iterator = this.books.iterator();
   while (iterator.hasNext()) {
   Book book = iterator.next();
   book.setAuthor(null);
   iterator.remove();
   }
   }
   ```
5. override equals() and hashCode()

   By properly overriding equals() and hashcode() methods the application obtains the same results across all entity state transitions

   for @OneToMany associations these methods should be override on the child side

   incase we use the auto-generated database identifier to override these two methods.

   The most important aspect to keep in mind is that, for autogenerated database identifiers, the equals() method should perform a null

   check on the identifier before performing an equality check and the hashCode() method should return a constant value

   Since the Book entity is on the child side we highlight these two aspects as follows

   ```
   @Override
   public boolean equals(Object obj) {
   ...
   return id != null && id.equals(((Book) obj).id);
   }
   @Override
   public int hashCode() {
   return 2021;
   }
   ```
6. Use lazy fetching on both sides of the association

   by default fetching a parent will not fetch children entities

   This means @OneToMany is lazy

   On the other hand, fetching a child entity will eagerly fetch its parent by default

   Its advisable to explicitly set @ManyToOne to lazy and rey on eager fetching only on query basis

   in this case we Book entity explicitly maps @ManyToOne as lazy

   @ManyToOne(fetch =  FetchType.LAZY)
7. Pay attention to how toString() is overridden

   if toString() is to be overridden, then be sure to involve the basic attributes fetched when the entity is loaded from the database

   involving lazy attributes or associations will trigger separate SQL statements that fetch the corresponding data and throw LazyInitializationException.

   example when we implement the toString() method for Author entity then we dont mention the books collection, we mention only the basic attributes

   (id, name, age and genre)

   ```
   @Override
   public String toString() {
   return "Author{" + "id=" + id + ", name=" + name 
   + ", genre=" + genre + ", age=" + age + '}';
   }
   ```
8. Use @JoinColumn to specify the Join Column Name

   The join column defined by the owner entity (Book) stores the ID value and has a foreign key to the Author entity

   its advisable to specify the desired name for this column

   This way you avoid potential confusions/mistakes when referring to it (e.g in native queries)

   In this case we add @JoinColumn to the book entity as follows

   `@JoinColumn(name = "author_id)`

pay attention to remove entities operations especially child entities operations

While CascadeType.Remove and orphanRemoval=true will do their jobs but they may produce too may SQL statements

Relying on bulk operations is the best way to delete any significant amount of entities

## Why should you avoid the unidirectional @OneToMany Association

Consider the Author and Book entities are involved in a bidirectional lazy @OneToMany association

Trying to insert a child entity, a book will result in one SQL INSERT statement triggered against the book table

Trying to delete will result in one SQL delete statement triggered against the book table

Now lets assume that the same Author and Book entities are involved in a unidirectional @OneToMany association

```
@OneToMany(cascade = CascadeType.ALL, orphanRemoval = true)
private Listbook books = new ArrayList<>();
```

The missing @ManyToOne association leads to separate junction table(author_books) meant to manage the parent-child association

example

![image.png](assets/imagetwo.png)

The junction table holds two foreign keys, so indexing consumes more memory than in the case of bidirectional relationship @OneToMany

Moreover having three tables affects the query operations as well

Reading the data may require three joins instead of two as in the case of bidirectional @ManyToOne association

lets see how INSERT AND DELETE act in unidirectional relationship

### Regular Unidirectional @OneToMany

checking how the INSERT and

#### Persisting Author and their books

The service method for persisting an author and the associated books is as follows

```
@Transactional
public void insertAuthorWithBooks() {
Author jn = new Author();
jn.setName("Joana Nimar");
jn.setAge(34);
jn.setGenre("History");
Book jn01 = new Book();
jn01.setIsbn("001-JN");
jn01.setTitle("A History of Ancient Prague");
Book jn02 = new Book();
jn02.setIsbn("002-JN");
jn02.setTitle("A People's History");
Book jn03 = new Book();
jn03.setIsbn("003-JN");
jn03.setTitle("World History");
jn.addBook(jn01);
jn.addBook(jn02);
jn.addBook(jn03);
authorRepository.save(jn);
}
```

inspecting the generated SQL statements reveals that in comparison to the bidirectional @OneToMany association there are 3 additional insert statements in the junction table (for n books there are n additional inserts)

```
INSERT INTO author (age, genre, name)
VALUES (?, ?, ?)
Binding:[34, History, Joana Nimar]
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[001-JN, A History of Ancient Prague]
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[002-JN, A People's History]
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[003-JN, World History]
```

-- additional inserts that are not needed for bidirectional @OneToMany

```
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 1]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 2]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 3]
```

so in this context the unidirectional @OneToMany association is less efficient than bidirectional @OneToMany association

#### Persisting a new book of an existing author

Since Joana Nimar has just published a new book, we have to add it to the book table. This time, the service-method looks as follows:

@Transactional
public void insertNewBook() {
Author author = authorRepository.fetchByName("Joana Nimar");
Book book = new Book();
book.setIsbn("004-JN");
book.setTitle("History Details");
author.addBook(book); // use addBook() helper
authorRepository.save(author);
}

Calling this method and focusing on SQL INSERT statements results in the following output:

```
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[004-JN, History Details]
-- the following DML statements don't appear in bidirectional @OneToMany
DELETE FROM author_books
WHERE author_id = ?
Binding:[1]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 1]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 2]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)Binding:[1, 3]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 4]
```

So, in order to insert a new book, the JPA persistence provider (Hibernate) deletes all the associated books from the junction table and it adds the new book in-memory and persists again

This is far from being efficient and potential performance penalty is quite obvious

#### Deleting the last book

Deleting the last book involves fetching the associated List of an author and deleting the last book from this list, as follows:

```
@Transactional
public void deleteLastBook() {
Author author = authorRepository.fetchByName("Joana Nimar");
Listbook books = author.getBooks();
// use removeBook() helper
author.removeBook(books.get(books.size() - 1));
}
```

Calling deleteLastBook() reveals the following relevant SQL statements:

```
DELETE FROM author_books
WHERE author_id = ?
Binding:[1]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 1]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 2]
```

-- for bidirectional @OneToMany this is the only needed DML

```
DELETE FROM book
WHERE id = ?
Binding:[3]
```

So, in order to delete the last book, the JPA persistence provider (Hibernate) deletes all associated books from the junction table, removes in-memory the last book, and persists the remaining books back again.

So, in comparison to the bidirectional @OneToMany association, there are several additional DML statements representing a performance penalty.

The more associated books there are, the larger the performance penalty.

#### Deleting the first book

Deleting the first book involves fetching the associated List of an author and deleting the first book from this list, as follows:

```
@Transactional
public void deleteFirstBook() {
Author author = authorRepository.fetchByName("Joana Nimar");
Listbook books = author.getBooks();
author.removeBook(books.get(0));
}
```

Calling deleteFirstBook() reveals the following relevant SQL statements:

```
DELETE FROM author_books
WHERE author_id = ?
Binding:[1]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 2]
INSERT INTO author_books (author_id, books_id)
VALUES (?, ?)
Binding:[1, 3]
```

-- for bidirectional @OneToMany this is the only needed DML

```
DELETE FROM book
WHERE id = ?
Binding:[1]
```

So, deleting the first book acts exactly as deleting the last book.

Besides the performance penalties caused by the dynamic number of additional SQL statements, we also face the performance penalties caused by the deletion and reinsertion of the index entries associated with the foreign key column of the junction table (most databases use indexes for foreign key columns).

When the database deletes all the table rows associated with the parent entity from the junction table, it also deletes the corresponding index entries. When the database inserts back in the junction table, it inserts the index entries as well.

### Using @OrderColumn

By adding the @OrderColumn annotation, the unidirectional @OneToMany association becomes ordered. In other words, @OrderColumn instructs Hibernate to materialize the element index (index of every collection element) into a separate database column of the junction table so that the collection is sorted using an ORDER BY clause. In this case, the index of every collection element is going to be stored in the books\_order column of the junction table.

#### Persist the Author and Books

Persisting the author and the associated books from the snapshot via the insertAuthorWithBooks() service-method triggers the following relevant SQL statements:

```
INSERT INTO author (age, genre, name)
VALUES (?, ?, ?)
Binding:[34, History, Joana Nimar]
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[001-JN, A History of Ancient Prague]
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[002-JN, A People's History]
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[003-JN, World History]
```

-- additional inserts not needed for bidirectional @OneToMany

```
INSERT INTO author_books (author_id, books_order, books_id)
VALUES (?, ?, ?)
Binding:[1, 0, 1]
INSERT INTO author_books (author_id, books_order, books_id)
VALUES (?, ?, ?)
Binding:[1, 1, 2]
INSERT INTO author
_books (author_id, books_order, books_id)
VALUES (?, ?, ?)
Binding:[1, 2, 3]
```

Looks like @OrderColumn doesn’t bring any benefit. The three additional INSERT statements are still triggered.

#### Persist a New Book of an Existing Author

Persisting a new book via the insertNewBook() service-method triggers the following relevant SQL statements:

```
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[004-JN, History Details]
-- this is not needed for bidirectional @OneToMany
INSERT INTO author_books (author_id, books_order, books_id)
VALUES (?, ?, ?)
Binding:[1, 3, 4]
```

There is good news and bad news!

The good news is that, this time, Hibernate doesn’t delete the associated books to add them back from memory.

The bad news is that, in comparison to bidirectional @OneToMany association, there is still an additional INSERT statement in the junction table. So, in this context, @OrderColumn brought some benefit.

#### Delete the Last Book

Deleting the last book via deleteLastBook() triggers the following relevant SQL statements:

```
DELETE FROM author_books
WHERE author_id = ?
AND books_order = ?
Binding:[1, 2]
```

-- for bidirectional @OneToMany this is the only needed DML

```
DELETE FROM book
WHERE id = ?
Binding:[3]
```

Looks like @OrderColumn brought some benefit in the case of removing the last book. The JPA persistence provider (Hibernate) did not delete all the associated books to add the remaining back from memory.

But, in comparison to the bidirectional @OneToMany association, there is still an additional DELETE triggered against the junction table.

#### Delete the First Book

Deleting the first book via deleteFirstBook() triggers the following relevant SQL statements:

```
DELETE FROM author_books
WHERE author_id = ?
AND books_order = ?
Binding:[1, 2]
UPDATE author_books
SET books_id = ?
WHERE author_id = ?
AND books_order = ?
Binding:[3, 1, 1]
UPDATE author_books
SET books_id = ?
WHERE author_id = ?
AND books_order = ?
Binding:[2, 1, 0]
```

-- for bidirectional @OneToMany this is the only needed DML

```
DELETE FROM book
WHERE id = ?
Binding:[1]
```

The more you move away from the end of the collection, the smaller the benefit of using @OrderColumn. Deleting the first book results in a DELETE from the junction table followed by a bunch of UPDATE statements meant to preserve the in-memory order of the collection in the database. Again, this is not efficient.

Adding @OrderColumn can bring some benefits for removal operations. Nevertheless, the closer an element to be removed is to the head of the fetched list, the more UPDATE statements are needed.

This causes performance penalties. Even in the best-case scenario (removing an element from the tail of the collection), this approach is not better than bidirectional @OneToMany association.

### @JoinColumn

Now, let’s see if adding @JoinColumn will bring any benefit:

```
@OneToMany(cascade = CascadeType.ALL, orphanRemoval = true)
@JoinColumn(name = "author_id")
private Listbook books = new ArrayList<>();
```

Adding @JoinColumn instructs Hibernate that the @OneToMany association is capable of
controlling the child-table foreign key. In other words, the junction table is eliminated
and the number of tables is reduced from three to two, as shown

![image.png](assets/imagefour.png)

#### Persist the author and the books

Persisting the author and the associated books via the insertAuthorWithBooks() service-method triggers the following relevant SQL statements:

```
INSERT INTO author (age, genre, name)
VALUES (?, ?, ?)
Binding:[34, History, Joana Nimar]                                                                                                                         INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[001-JN, A History of Ancient Prague]
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[002-JN, A People's History]
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[003-JN, World History]
```

-- additional DML that are not needed in bidirectional @OneToMany

```
UPDATE book
SET author_id = ?
WHERE id = ?
Binding:[1, 1]
UPDATE book
SET author_id = ?
WHERE id = ?
Binding:[1, 2]
UPDATE book
SET author_id = ?
WHERE id = ?
Binding:[1, 3]
```

So, for each inserted book, Hibernate triggers an UPDATE to set the author\_id value. Obviously, this adds a performance penalty in comparison to the bidirectional @OneToMany association.

#### Persist a New Book of an Existing Author

Persisting a new book via the insertNewBook() service-method triggers the following relevant SQL statements:

```
INSERT INTO book (isbn, title)
VALUES (?, ?)
Binding:[004-JN, History Details]
```

-- additional DML that is not needed in bidirectional @OneToMany

```
UPDATE book
SET author_id = ?
WHERE id = ?
Binding:[1, 4]
```

This is not as bad as a regular unidirectional @OneToMany association, but it still requires an UPDATE statement that is not needed in bidirectional @OneToMany associations.

#### Delete the Last Book

Deleting the last book via deleteLastBook() triggers the following relevant SQL statements:

```
UPDATE book
SET author_id = NULL
WHERE author_id = ?
AND id = ?
Binding:[1, 3]
```

-- for bidirectional @OneToMany this is the only needed DML

```
DELETE FROM book
WHERE id = ?
Binding:[3]
```

The JPA persistence provider (Hibernate) dissociates the book from its author by setting author\_id to null.

Next, the disassociated book is deleted, thanks to orphanRemoval=true. Nevertheless, this additional UPDATE is not necessary with bidirectional @OneToMany association.

#### Delete the First Book

Deleting the first book via deleteFirstBook() triggers the following relevant SQL statements (these are the same SQL statements as in the previous subsection):

```
UPDATE book
SET author_id = NULL
WHERE author_id = ?
AND id = ?
Binding:[1, 1]
```

-- for bidirectional @OneToMany this is the only needed DML

```
DELETE FROM book
WHERE id = ?
Binding:[1]
```

The UPDATE is still there! Once again, the bidirectional @OneToMany association wins this game.

Adding @JoinColumn can provide benefits over the regular unidirectional @OneToMany, but is not better than a bidirectional @OneToMany association. The additional UPDATE statements still cause a performance degradation

Adding @JoinColumn and @OrderColumn at the same time is still not better than bidirectional @OneToMany.

Moreover, using Set instead of List or unidirectional @OneToMany with @JoinColumn (e.g., @ManyToOne @Join Column(name = "author\_id", updatable = false, insertable = false)) still performs worse than a bidirectional @OneToMany association.

As a rule of thumb, a unidirectional @OneToMany association is less efficient than a bidirectional @OneToMany or unidirectional @ManyToOne associations.

## How Efficient Is the Unidirectional @ManyToOne

Let’s assume that Author and Book are involved in a unidirectional lazy @ManyToOne association.

In code, the Author and Book entities are as follows:

```
@Entity
public class Author implements Serializable {
private static final long serialVersionUID = 1L;
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;
private String name;
private String genre;
private int age;
...
}@Entity
public class Book implements Serializable {
private static final long serialVersionUID = 1L;
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;
private String title;
private String isbn;
@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = "author_id")
private Author author;
...
}
```

### Adding a New Book to a Certain Author

The most efficient way to add a new book to a certain author is shown in the following

```
@Transactional
public void insertNewBook() {
Author author = authorRepository.getOne(4L);
Book book = new Book();
book.setIsbn("003-JN");
book.setTitle("History Of Present");
book.setAuthor(author);
bookRepository.save(book);
}
```

This method will trigger a single INSERT SQL statement. The author\_id column will be populated with the identifier of the associated Author entity:

```
INSERT INTO book (author_id, isbn, title)
VALUES (?, ?, ?)
Binding:[4, 003-JN, History Of Present]
```

Notice that we used the getOne() method, which returns an Author reference via EntityManager.getReference()

The reference state may be lazily fetched, but you don’t need it in this context. Therefore, you avoid an unneeded SELECT statement. Of course, relying on findById() is also possible and desirable if you need to actually load the Author instance in the Persistence Context. Obviously, this will happen via a SELECT statement.

The Hibernate Dirty Checking mechanism works as expected

In other words, updating the book will result in UPDATE statements triggered on your behalf.

```
@Transactional
public void insertNewBook() {
Author author = authorRepository.getOne(4L);
Book book = new Book();
book.setIsbn("003-JN");
book.setTitle("History Of Present");
book.setAuthor(author);
bookRepository.save(book);
book.setIsbn("not available");
}
```

This time, calling insertNewBook() will trigger an INSERT and an UPDATE:

```
INSERT INTO book (author_id, isbn, title)
VALUES (?, ?, ?)
UPDATE book
SET author_id = ?,
isbn = ?,
title = ?
WHERE id = ?
```

Since Hibernate populates the author\_id column with the identifier of the associated Author entity, adding a new book to a certain author is efficient.

### Fetching All Books of an Author

You can fetch all the books written by an author via a JPQL query as follows:

```
@Transactional(readOnly = true)
@Query("SELECT b FROM Book b WHERE b.author.id = :id")
Listbook fetchBooksOfAuthorById(Long id);
```

Calling fetchBooksOfAuthorById() from a service-method is quite simple:

```
public void fetchBooksOfAuthorById() {
Listbook books = bookRepository.fetchBooksOfAuthorById(4L);
}
```

The triggered SELECT is shown here:

```
SELECT
book0_.id AS id1_1_,
book0_.author_id AS author_i4_1_,
book0_.isbn AS isbn2_1_,
book0_.title AS title3_1_
FROM book book0_
WHERE book0_.author_id = ?
```

Modifying a book will take advantage of the Dirty Checking mechanism. In other words, updating a book from this collection will result in a UPDATE statement triggered on your behalf. Check out the following code:

```
@Transactional
public void fetchBooksOfAuthorById() {
Listbook books = bookRepository.fetchBooksOfAuthorById(4L);
books.get(0).setIsbn("not available");
}
```

This time, calling fetchBooksOfAuthorById() will trigger a SELECT and an UPDATE:

```
SELECT
book0_.id AS id1_1_,
book0_.author_id AS author_i4_1_,
book0_.isbn AS isbn2_1_,
book0_.title AS title3_1_
FROM book book0_
WHERE book0_.author_id = ?
```

```
UPDATE book
SET author_id = ?,
isbn = ?,
title = ?
WHERE id = ?
```

Fetching all books of an author requires a single SELECT; therefore, this operation is efficient

The fetched collection is not managed by Hibernate but adding/removing books is quite efficient and easy to accomplish.

### Paging the Books of an Author

Fetching all books work fine as long as the number of child records is rather small. Generally speaking, fetching large collections is definitely a bad practice that leads to important performance penalties.

Pagination comes to the rescue as follows

```
@Transactional(readOnly = true)
@Query("SELECT b FROM Book b WHERE b.author.id = :id")
Pagebook fetchPageBooksOfAuthorById(Long id, Pageable pageable);
```

You can call fetchPageBooksOfAuthorById()from a service-method

```
public void fetchPageBooksOfAuthorById() {
Pagebook books = bookRepository.fetchPageBooksOfAuthorById(4L,
PageRequest.of(0, 2, Sort.by(Sort.Direction.ASC, "title")));
books.get().forEach(System.out::println);
}
```

This method triggers two SELECT statements:

```
SELECT
book0_.id AS id1_1_,
book0_.author_id AS author_i4_1_,
book0_.isbn AS isbn2_1_,
book0_.title AS title3_1_
FROM book book0_
WHERE book0_.author_id = ?
ORDER BY book0_.title ASC LIMIT ?SELECT
COUNT(book0_.id) AS col_0_0_
FROM book book0_
WHERE book0_.author_id = ?
```

the fetched collection is not managed by Hibernate, but modifying a book will take advantage of the Dirty Checking mechanism.

### Fetching All Books of an Author and Adding a New Book

The section “Fetching All Books of an Author” already covered half of this topic while the section “Adding a New Book to a Certain Author” covered the other half. Joining these sections results in the following code:

```
@Transactional
public void fetchBooksOfAuthorByIdAndAddNewBook() {
Listbook books = bookRepository.fetchBooksOfAuthorById(4L);
Book book = new Book();
book.setIsbn("004-JN");
book.setTitle("History Facts");
book.setAuthor(books.get(0).getAuthor());
books.add(bookRepository.save(book));
}
```

The triggered SQL statements are:

```
SELECT
book0_.id AS id1_1_,
book0_.author_id AS author_i4_1_,
book0_.isbn AS isbn2_1_,
book0_.title AS title3_1_
FROM book book0_
WHERE book0_.author_id = ?
```

```
INSERT INTO book (author_id, isbn, title)
VALUES (?, ?, ?)
```

Since fetching all the books of an author requires a single SELECT and adding a new book into the fetched collection requires a single INSERT, this operation is efficient.

### Fetching all Books of an Author and Deleting a Book

The following code fetches all books of an author and deletes the first book:

```
@Transactional
public void fetchBooksOfAuthorByIdAndDeleteFirstBook() {
Listbook books = bookRepository.fetchBooksOfAuthorById(4L);
bookRepository.delete(books.remove(0));
}
```

Besides the well known SELECT needed to fetch all books of the author, deletion takes place in a single DELETE statement, as follows:

```
DELETE FROM book
WHERE id = ?
```

Since fetching all books of an author requires a single SELECT and removing a book from the fetched collection requires a single DELETE, this operation is efficient.

It looks like unidirectional @ManyToOne association is quite efficient and it can be used whenever a bidirectional @OneToMany association is not needed. Again, try to avoid the unidirectional @OneToMany association

## How to Effectively Shape the @ManyToMany Association

This time, the well-known Author and Book entities are involved in a bidirectional lazy @ManyToMany association (an author has written more books and a book was written by several authors).

![image.png](assets/imagefive.png)

The bidirectional @ManyToMany association can be navigated from both sides, therefore, both sides can be parents (parent-side).

Since both are parents, none of them will hold a foreign key. In this association, there are two foreign keys that are stored in a separate table, known as the junction or join table. The junction table is hidden and it plays the child-side role.

### The best way to code a bidirectional @ManyToMany association is described in the following sections.

#### Choose the Owner of the Relationship

Using the default @ManyToMany mapping requires the developer to choose an owner of the relationship and a mappedBy side (aka, the inverse side).

Only one side can be the owner and the changes are only propagated to the database from this particular side.

For example, Author can be the owner, while Book adds a mappedBy side.

```
@ManyToMany(mappedBy = "books")
private Setauthor authors = new HashSet<>();
```

#### Always Use Set not List

Especially if remove operations are involved, it is advisable to rely on Set and avoid List.

Set performs much better than List.

```
private Setbook books = new HashSet<>(); // in Author
private Setauthor authors = new HashSet<>(); // in Book
```

#### Keep Both Sides of the Association in Sync

You can easily keep both sides of the association in sync via helper methods added on the side that you are more likely to interact with. For example, if the business logic is more interested in manipulating Author than Book then the developer can add Author to least these three helpers:

addBook(), removeBook() and removeBooks().

#### Avoid CascadeType.ALL and CascadeType.REMOVE

In most of the cases, cascading removals are bad ideas. For example, removing an Author entity should not trigger a Book removal because the Book can be referenced by other authors as well

So, avoid CascadeType.ALL and CascadeType.REMOVE and rely on explicit CascadeType.PERSIST and CascadeType.MERGE:

```
@ManyToMany(cascade = {CascadeType.PERSIST, CascadeType.MERGE})
private Setbook books = new HashSet<>();
```

The orphan removal (orphanRemoval) option is defined on @OneToOne and @OneToMany relationship annotations, but on neither of the @ManyToOne or @ManyToMany annotations

#### Setting Up the Join Table

Explicitly setting up the join table name and the columns names allows the developer to reference them without confusion. This can be done via @JoinTable as in the following example:

```
@JoinTable(name = "author_book",
joinColumns = @JoinColumn(name = "author_id"),
inverseJoinColumns = @JoinColumn(name = "book_id")
)
```

#### Using Lazy Fetching on Both Sides of the Association

By default, the @ManyToMany association is lazy. Keep it this way! Don’t do this: @ManyToMany(fetch=FetchType.EAGER)

#### Override equals() and hashCode()

By properly overriding the equals() and hashCode() methods, the application obtains the same results across all entity state transitions.

For bidirectional @ManyToMany associations, these methods should be overridden on both sides.

#### Pay Attention to How toString() Is Overridden

If toString() needs to be overridden, involve only the basic attributes fetched when the entity is loaded from the database. Involving lazy attributes or associations will trigger separate SQL statements for fetching the corresponding data.

Alternatively, @ManyToMany can be replaced with two bidirectional @OneToMany associations. In other words, the junction table can be mapped to an entity.

## Why Set Is Better than List in @ManyToMany

First of all, keep in mind that Hibernate deals with @ManyToMany relationships as two unidirectional @OneToMany associations.

The owner-side and the child-side (the junction table) represents one unidirectional @OneToMany association.

On the other hand, the non-owner-side and the child-side (the junction table) represent another unidirectional @OneToMany association.

Each association relies on a foreign key stored in the junction table

In the context of this statement, the entity removal (or reordering) results in deleting all junction entries from the junction table and reinserts them to reflect the memory content (the current Persistence Context content).

### Using List

Let’s assume that Author and Book involved in a bidirectional lazy @ManyToMany association are mapped via java.util.List, as shown here (only the relevant code is listed):

```
@Entity
public class AuthorList implements Serializable {
...
@ManyToMany(cascade = {CascadeType.PERSIST, CascadeType.MERGE})
@JoinTable(name = "author_book_list",
joinColumns = @JoinColumn(name = "author_id"),
inverseJoinColumns = @JoinColumn(name = "book_id")
)
private Listbooklist books = new ArrayList<>();
...
}
```

```
@Entity
public class BookList implements Serializable {
...
@ManyToMany(mappedBy = "books")
private Listauthorlist authors = new ArrayList<>();
...
}
```

![image.png](assets/imagesix.png?t=1699124868575)

The goal is to remove the book called One Day (the book with ID of 2) written by the author, Alicia Tom (the author with ID 1). Considering that the entity representing this author is stored via a variable named alicia, and the book is stored via a variable named oneDay, the deletion can be done via removeBook() as follows:

alicia.removeBook(oneDay);

The SQL statements triggered by this deletion are:

```
DELETE FROM author_book_list
WHERE author_id = ?
Binding: [1]
INSERT INTO author_book_list (author_id, book_id)
VALUES (?, ?)
Binding: [1, 1]
INSERT INTO author_book_list (author_id, book_id)
VALUES (?, ?)
Binding: [1, 3]
```

So, the removal didn’t materialized in a single SQL statement. Actually, it started by deleting all junction entries of alicia from the junction table. Further, the junction entries that were not the subject of removal were reinserted to reflect the in-memory content (Persistence Context). The more junction entries reinserted, the longer the database transaction.

### Using set

Consider switching from List to Set as follows:

```
@Entity
public class AuthorSet implements Serializable {
...
@ManyToMany(cascade = {CascadeType.PERSIST, CascadeType.MERGE})
@JoinTable(name = "author_book_set",
joinColumns = @JoinColumn(name = "author_id"),
inverseJoinColumns = @JoinColumn(name = "book_id")
)
private Setbookset books = new HashSet<>();
...
}
```

```
@Entity
public class BookSet implements Serializable {
...
@ManyToMany(mappedBy = "books")
private Setauthorset authors = new HashSet<>();
...
}
```

This time, calling alicia.removeBook(oneDay) will trigger the following SQL DELETE statement:

```
DELETE FROM author_book_set
WHERE author_id = ?
AND book_id = ?
Binding: [1, 2]
```

This is much better since a single DELETE statement is needed to accomplish the job.

When using the @ManyToMany annotation, always use a java.util.Set. Do not use the java.util.List. In the case of other associations, use the one that best fits your case. If you go with List, do not forget to be aware of the HHH-58557  issue that was fixed starting with Hibernate 5.0.8

### Preserving the Order of the Result Set

It’s a well-known fact that java.util.ArrayList preserves the order of inserted elements while java.util.HashSet doesn’t.

In other words, java.util.ArrayList has a predefined entry order of elements, while java.util.HashSet is, by default, unordered

There are at least two ways to order the result set by the given columns defined by JPA specification

1. Use @OrderBy to ask the database to order the fetched data by the given columns (appends the ORDER BY clause in the generated SQL query to retrieve the entities in a specific order) and Hibernate to preserve this order
2. Use @OrderColumn to permanently order this via an extra column (in this case, stored in the junction table).

This annotation (@OrderBy) can be used with @OneToMany/@ManyToMany associations and @ElementCollection. Adding @OrderBy without an explicit column will result in ordering the entities ascending by their primary key (ORDER BY author1\_.id ASC).

Ordering by multiple columns is possible as well (e.g., order descending by age and ascending by name, @OrderBy("age DESC, name ASC"). Obviously, @OrderBy can be used with java.util.List as well.

#### Using @OrderBy

Consider the data snapshot in Figure

![image.png](assets/imageseven.png)

There is a book written by six authors. The goal is to fetch the authors in descending order by name via Book#getAuthors(). This can be done by adding @OrderBy in Book, as shown here:

```
@ManyToMany(mappedBy = "books")
@OrderBy("name DESC")
private Setauthor authors = new HashSet<>();
```

When getAuthors() is called, the @OrderBy will:

1. Attach the corresponding ORDER BY clause to the triggered SQL. This will instruct the database to order the fetched data.
2. Signal to Hibernate to preserve the order. Behind the scenes, Hibernate will preserve the order via a LinkedHashSet

So, calling getAuthors() will result in a Set of authors conforming to the @OrderBy information. The triggered SQL is the following SELECT containing the ORDER BY clause:

```
SELECT
authors0_.book_id AS book_id2_1_0_,
authors0_.author_id AS author_i1_1_0_,
author1_.id AS id1_0_1_,
author1_.age AS age2_0_1_,
author1_.genre AS genre3_0_1_,
author1_.name AS name4_0_1_
FROM author_book authors0_
INNER JOIN author author1_
ON authors0_.author_id = author1_.id
WHERE authors0_.book_id = ?
ORDER BY author1_.name DESC
```

Displaying Set will output the following (via Author#toString()):

```
Author{id=2, name=Quartis Young, genre=Anthology, age=51},
Author{id=6, name=Qart Pinkil, genre=Anthology, age=56},
Author{id=5, name=Martin Leon, genre=Anthology, age=38},
Author{id=1, name=Mark Janel, genre=Anthology, age=23},
Author{id=4, name=Katy Loin, genre=Anthology, age=56},
Author{id=3, name=Alicia Tom, genre=Anthology, age=38}
```

Using @OrderBy with HashSet will preserve the order of the loaded/fetched Set, but this is not consistent across the transient state. If this is an issue, to get consistency across the transient state as well, consider explicitly using LinkedHashSet instead of HashSet. So, for full consistency, use:

```
@ManyToMany(mappedBy = "books")
@OrderBy("name DESC")
private Setauthor authors = new LinkedHashSet<>();
```

## Why and When to Avoid Removing Child Entities with CascadeType.Remove and orphanRemoval=true

First of all, let’s quickly highlight the differences between CascadeType.REMOVE and orphanRemoval=true

Let’s use the Author and Book entities involved in a bidirectional lazy @OneToMany association, written as follows:

```
// in Author.java
@OneToMany(cascade = CascadeType.ALL,
mappedBy = "author", orphanRemoval = true)                                                                                                               private List books = new ArrayList<>();// in Book.java
@ManyToOne(fetch = FetchType.LAZY)
@JoinColumn(name = "author_id")
private Author author;
```

Removing an Author entity is automatically cascaded to the associated Book entities. This is happening as long as CascadeType.REMOVE or orphanRemoval=true is present. In other words, from this perspective, the presence of both is redundant.

Then how are they different? Well, consider the following helper-method used to disconnect (or disassociate) a Book from its Author:

```
public void removeBook(Book book) {
book.setAuthor(null);
this.books.remove(book);
}
```

Or, to disconnect all Books from their Authors:

```
public void removeBooks() {
Iteratorbook iterator = this.books.iterator();
while (iterator.hasNext()) {
Book book = iterator.next();
book.setAuthor(null);
iterator.remove();
}
}
```

Calling the removeBook() method in the presence of orphanRemoval=true will result in automatically removing the book via a DELETE statement.

Calling it in the presence of orphanRemoval=false will trigger an UPDATE statement.

Since disconnecting a Book is not a remove operation, the presence of CascadeType.REMOVE doesn’t matter.

So, orphanRemoval=true is useful for cleaning up entities (remove dangling references) that should not exist without a reference from an owner entity (Author).

But how efficient are these settings? The short answer is: not very efficient if they must affect a significant number of entities.

The long answer starts by deleting an author in the following service-method (this author has three associated books):

```
@Transactional
public void deleteViaCascadeRemove() {
Author author = authorRepository.findByName("Joana Nimar");
authorRepository.delete(author);
}
```

Deleting an author will cascade the deletion to the associated books. This is the effect of CascadeType.ALL, which includes CascadeType.REMOVE. But, before deleting the associated books, they are loaded in the Persistence Context via a SELECT. If they are already in the Persistence Context then they are not loaded. If the books are not present in the Persistence Context then CascadeType.REMOVE will not take effect. Further, there are four DELETE statements, one for deleting the author and three for deleting the associated books:

```
DELETE
FROM book
WHERE id=?
Binding:[1]
DELETE
FROM book
WHERE id=?
Binding:[2]
DELETE
FROM book
WHERE id=?
Binding:[4]
DELETE
FROM author
WHERE id=?
Binding:[4]
```

For each book there is a separate DELETE statement. The more books there are to delete, the more DELETE statements you have and the larger the performance penalty.

Now let’s write a service-method that deletes based on the orphanRemoval=true. For the sake of variation, this time, we load the author and the associated books in the same SELECT.

```
@Transactional
public void deleteViaOrphanRemoval() {
Author author = authorRepository.findByNameWithBooks("Joana Nimar");
author.removeBooks();
authorRepository.delete(author);
}
```

Unfortunately, this approach will trigger the exact same DELETE statements as in the case of cascading the deletes, so it’s prone to the same performance penalties.

If your application triggers sporadic deletes, you can rely on CascadeType.REMOVE and/or orphanRemoval=true. This is useful especially if you delete managed entities, so you need Hibernate to manage the entities state transitions. Moreover, via this approach, you benefit from the automatic Optimistic Locking mechanism (e.g., @Version) for parents and children. But, if you are just looking for approaches that delete more efficiently (in fewer DML statements), we will consider a few of them. Of course, each approach has its own trade-offs

The following four approaches delete the authors and the associated books via bulk operations. This way, you can optimize and control the number of triggered DELETE statements. These operations are very fast, but they have three main shortcomings:

1. They ignore the automatic Optimistic Locking mechanism (for example, you cannot rely on @Version anymore)
2. The Persistence Context is not synchronized to reflect the modifications performed by the bulk operations, which may lead to an outdated context
3. They don’t take advantage of cascading removals (CascadeType. REMOVE) or orphanRemoval

If these shortcomings matter to you, you have two options: avoid bulk operations or explicitly deal with the problem. The most difficult part is to emulate the job of the automatic Optimistic Locking mechanism for children that are not loaded in the Persistence Context.

The following examples assume that there is no automatic Optimistic Locking mechanism enabled. However, they manage the Persistence Context synchronization issues via flushAutomatically = true and clearAutomatically = true. Do not conclude that these two settings are always needed. Their usage depends on what you want to achieve.

### Deleting Authors that Are Already Loaded in the Persistence Context

Let’s tackle the case when, in the Persistence Context, there is only one Author loaded and the case when there are more Authors loaded, but not all of them. The associated books (which are or aren’t already loaded in the Persistence Context) have to be deleted as well.

### One Author Has Already Been Loaded in the Persistence Context

Let’s assume that the Author that should be deleted was loaded earlier in the Persistence Context without their associated Book. To delete this Author and the associated books, you can use the author identifier (author.getId()). First, delete all the author’s associated books:

```
// add this method in BookRepository@Transactional
@Modifying(flushAutomatically = true, clearAutomatically = true)
@Query("DELETE FROM Book b WHERE b.author.id = ?1")
public int deleteByAuthorIdentifier(Long id);
```

Then, let’s delete the author by his identifier:

```
// add this method in AuthorRepository
@Transactional
@Modifying(flushAutomatically = true, clearAutomatically = true)
@Query("DELETE FROM Author a WHERE a.id = ?1")
public int deleteByIdentifier(Long id);
```

The presence of flushAutomatically = true, clearAutomatically = true is explained a little bit later. For now, the service-method responsible for triggering the deletion is:

```
@Transactional
public void deleteViaIdentifiers() {
Author author = authorRepository.findByName("Joana Nimar");
bookRepository.deleteByAuthorIdentifier(author.getId());
authorRepository.deleteByIdentifier(author.getId());
}
```

Calling deleteViaIdentifiers() triggers the following queries:

```
DELETE FROM book
WHERE author_id = ?
```

Notice that the associated books are not loaded in the Persistence Context and there are only two DELETE statements triggered. The number of books doesn’t affect the number of DELETE statements. The author can be deleted via the built-in deleteInBatch(Iterable entities) as well:

authorRepository.deleteInBatch(List.of(author));

### More Authors Have Been Loaded in the Persistence Context

Let’s assume that the Persistence Context contains more Authors that should be deleted. For example, let’s delete all Authors of age 34 fetched as a List (let’s assume that there are two authors of age 34).

Trying to delete by author identifier (as in the previous case) will result in a separate DELETE for each author. Moreover, there will be a separate DELETE for the associated books of each author. So this is not efficient.

This time, let’s rely on two bulk operations. One defined by you via the IN operator (which allows you to specify multiple values in a WHERE clause) and the built-in deleteInBatch(Iterable entities):

```
// add this method in BookRepository
@Transactional
@Modifying(flushAutomatically = true, clearAutomatically = true)
@Query("DELETE FROM Book b WHERE b.author IN ?1")
public int deleteBulkByAuthors(Listauthor authors);
```

The service-methods to delete a List and the associated Book are as follows:

```
@Transactional
public void deleteViaBulkIn() {
Listauthor authors = authorRepository.findByAge(34);
bookRepository.deleteBulkByAuthors(authors);
authorRepository.deleteInBatch(authors);
}
```

Calling deleteViaBulkIn() triggers the following queries:

```
DELETE FROM book
WHERE author_id IN (?, ?)
DELETE FROM author
WHERE id = ?
OR id = ?
```

Notice that the associated books are not loaded in the Persistence Context and there are only two DELETE statements triggered. The number of authors and books doesn’t affect the number of DELETE statements.

### One Author and His Associated Books Have Been Loaded in the Persistence Context

Assume that the Author (the one that should be deleted) and his associated Books are already loaded in the Persistence Context. This time there is no need to define bulk operations since the built-in deleteInBatch(Iterable entities) can do the job for you:

```
@Transactional
public void deleteViaDeleteInBatch() {
Author author = authorRepository.findByNameWithBooks("Joana Nimar");
bookRepository.deleteInBatch(author.getBooks());
authorRepository.deleteInBatch(List.of(author));
}
```

The main shortcoming here is the default behavior of the built-in deleteInBatch(Iterable entities), which, by default, don’t flush or clear the Persistence Context. This may leave the Persistence Context in an outdated state.

Of course, in the previous methods, there is nothing to flush before deletions and no need to clear the Persistence Context because, after the delete operations, the transaction commits.

Therefore the Persistence Context is closed. But, flush and clear (not necessarily both of them) are needed in certain cases. Commonly, the clear operation is needed much more often than the flush operation.

For example, the following method doesn’t need a flush prior to any deletions, but it needs a clear after any deletions. Otherwise it will cause an exception:

```
@Transactional
public void deleteViaDeleteInBatch() {
Author author = authorRepository.findByNameWithBooks("Joana Nimar");
bookRepository.deleteInBatch(author.getBooks());
authorRepository.deleteInBatch(List.of(author));
...
// later on, we forgot that this author was deleted
author.setGenre("Anthology");
}
```

The highlighted code will cause an exception of type:

```
org.springframework.orm.ObjectOptimisticLockingFailureException: Object of class [com.bookstore.entity.Author] with identifier [4]: Optimistic Locking failed; nested exception is org.hibernate.StaleObjectStateException: Row was updated or deleted by another transaction (or unsaved-value mapping was incorrect) : [com.bookstore.entity.Author#4]
```

Practically, the modification (the call of setGenre()) changes the Author entity contained in the Persistence Context, but this context is outdated since the author was deleted from the database.

In other words, after deleting the author and the associated books from the database, they will continue to exist in the current Persistence Context.

The Persistence Context is not aware of the deletions performed via deleteInBatch(Iterable entities). To make sure that the Persistence Context is cleared after the deletions, you can override the deleteInBatch(Iterable entities) to add @Modifying(clearAutomatically = true).

This way, the Persistence Context is automatically cleared after the deletions. If you are in a use case that requires a prior flush as well, then use @Modifying(flushAutomatically = true, clearAutomatically = true) or call the flush() method.

Or, even better, you can reuse the deleteViaIdentifiers() method, as shown here (we’ve already annotated this method with @Modifying(flushAutomatically = true, clearAutomatically = true)):

```
@Transactional
public void deleteViaIdentifiers() {
Author author = authorRepository.findByNameWithBooks("Joana Nimar");
bookRepository.deleteByAuthorIdentifier(author.getId());
authorRepository.deleteByIdentifier(author.getId());
}
```

Calling deleteViaIdentifiers() triggers the following queries:

```
DELETE FROM book
WHERE author_id = ?
DELETE FROM author
WHERE id = ?
```

The number of books doesn’t affect the number of DELETE statements.

If the Persistence Context manages several Authors and the associated Books that should be deleted then rely on the deleteViaBulkIn().

### Deleting When the Author and Books that Should Be Deleted Are Not Loaded in the Persistence Context

If the author that should be deleted and his associated books are not loaded in the Persistence Context then you can hardcode the author identifier (if you know it), as in the following service-method:

```
@Transactional
public void deleteViaHardCodedIdentifiers() {
bookRepository.deleteByAuthorIdentifier(4L);
authorRepository.deleteByIdentifier(4L);
}
```

The deleteByAuthorIdentifier() and deleteByIdentifier() methods are the same from “One Author Have Been Already Loaded in the Persistence Context” section. The triggered queries are quite obvious:

```
DELETE FROM book
WHERE author_id = ?
DELETE FROM author
WHERE id = ?
```

If there are more authors, you can use bulk operations to delete them:

```
// add this method in BookRepository
@Transactional
@Modifying(flushAutomatically = true, clearAutomatically = true)
@Query("DELETE FROM Book b WHERE b.author.id IN ?1")
public int deleteBulkByAuthorIdentifier(List<Long> id);// add this method in AuthorRepository

@Transactional
@Modifying(flushAutomatically = true, clearAutomatically = true)
@Query("DELETE FROM Author a WHERE a.id IN ?1")
public int deleteBulkByIdentifier(List<long> id);
```

Now, let’s delete two authors and their associated books:

```
@Transactional
public void deleteViaBulkHardCodedIdentifiers() {
Listlong authorsIds = Arrays.asList(1L, 4L);
bookRepository.deleteBulkByAuthorIdentifier(authorsIds);
authorRepository.deleteBulkByIdentifier(authorsIds);
}
```

The triggered SQL statements are as follows:

```
DELETE FROM book
WHERE author_id IN (?, ?)
DELETE FROM author
WHERE id IN (?, ?)
```

The number of authors and books doesn’t affect the number of DELETE statements. Since we don’t load anything in the Persistence Context, flushAutomatically = true, clearAutomatically = true has no effect.

In order to avoid outdated entities in the Persistence Context, do not forget to flush the EntityManager before the query is executed (flushAutomatically = true) and clear it after the query is executed (clearAutomatically = true).

If you don’t want/need to flush and/or clear then pay attention to how you manage to avoid outdated entities in Persistence Context.

As long as you know what you are doing, it’s not problematic to not flush and/or clear the Persistence Context. Ideally isolate bulk operations in dedicated transactional service-methods. This way, there is no need to explicitly flush and clear the Persistence Context. Issues may arise when you interleave bulk operations with managed entity operations.

The most efficient way to delete all entities is via the built-in deleteAllInBatch(), which trigger a bulk operation.

## How to Fetch Associations via JPA Entity Graphs

Now, in a nutshell, entity graphs (aka, fetch plans) were introduced in JPA 2.1 and they help you improve the performance of loading entities by solving lazy loading exceptions and N+1 issues.

The developer specifies the entity’s related associations and basic fields that should be loaded in a single SELECT statement. The developer can define multiple entity graphs for the same entity and can chain any number of entities, and even use sub-graphs to create complex fetch plans. Entity graphs are global and reusable across the entities (Domain Model).

To override the current FetchType semantics, there are two properties that you can set:

1. Fetch graph: This is the default fetching type represented by the javax.persistence.fetchgraph property. The attributes present in attributeNodes are treated as FetchType.EAGER. The remaining attributes are treated as FetchType.LAZY, regardless of the default/ explicit FetchType.
2. Load graph: This fetching type can be employed via the javax. persistence.loadgraph property. The attributes present in attributeNodes are treated as FetchType.EAGER. The remaining attributes are treated according to their specified or default FetchType.

An entity graph can be defined via annotations (e.g., @NamedEntityGraph)), via attributePaths (ad hoc entity graphs), and via the EntityManager API by calling the getEntityGraph() or createEntityGraph() methods.

Assume the Author and Book entities involved in a bidirectional lazy @OneToMany association. The entity graph (a fetch graph) should load all Authors and the associated Books in the same SELECT. The same thing can be obtained via JOIN FETCH, but this time let’s do it via entity graphs.

### Defining an Entity Graph via @NamedEntityGraph

The @NamedEntityGraph annotation occurs at entity-level. Via its elements, the developer can specify a unique name for this entity graph (via the name element) and the attributes to include when fetching the entity graph (via the attributeNodes element, which contains a list of @NamedAttributeNode annotations separated by commas; each @NamedAttributeNode from this list corresponds to a field/association that should be fetched).

Let’s put the entity graph in code in the Author entity:

```
@Entity
@NamedEntityGraph(
name = "author-books-graph",
attributeNodes = {
@NamedAttributeNode("books")
}
)
public class Author implements Serializable {
private static final long serialVersionUID = 1L;
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;
private String name;
private String genre;
private int age;
@OneToMany(cascade = CascadeType.ALL,
mappedBy = "author", orphanRemoval = true)
private Listbook books = new ArrayList<>();
// getters and setters omitted for brevity
}
```

Next, focus on the repository of the Author entity, AuthorRepository.

The AuthorRepository is the place where the entity graph should be specified. Spring Data provides support for entity graphs via the @EntityGraph annotation (the class of this annotation is org.springframework.data.jpa.repository.EntityGraph).

### Overriding a Query Method

For example, the code to use the entity graph (author-books-graph) to find all Authors, including the associated Book, is as follows

EntityGraph.EntityGraphType.FETCH is the default and indicates a fetch graph;

EntityGraph.EntityGraphType.LOAD can be specified for a load graph;

```
@Repository
@Transactional(readOnly = true)
public interface AuthorRepository extends JpaRepositoryauthor, {
@Override
@EntityGraph(value = "author-books-graph",
type = EntityGraph.EntityGraphType.FETCH)
public Listauthor findAll();
}
```

Calling the findAll() method will result in the following SQL SELECT statement:

```
SELECT
author0_.id AS id1_0_0_,
books1_.id AS id1_1_1_,
author0_.age AS age2_0_0_,
author0_.genre AS genre3_0_0_,
author0_.name AS name4_0_0_,
books1_.author_id AS author_i4_1_1_,
books1_.isbn AS isbn2_1_1_,
books1_.title AS title3_1_1_,
books1_.author_id AS author_i4_1_0__,
books1_.id AS id1_1_0__
FROM author author0_
LEFT OUTER JOIN book books1_
ON author0_.id = books1_.author_id
```

Notice that the generated query took into account the entity graph specified via @EntityGraph.

### Using the Query Builder Mechanism

Overriding findAll() is a convenient way to fetch all entities. But, use the Spring Data Query Builder mechanism to filter the fetched data via the WHERE clause. For example, you can fetch the entity graph for authors younger than the given age and in descending order by name as follows:

```
@Repository
@Transactional(readOnly = true)
public interface AuthorRepository extends JpaRepositoryauthor, {
@EntityGraph(value = "author-books-graph",
type = EntityGraph.EntityGraphType.FETCH)
public Listauthor findByAgeLessThanOrderByNameDesc(int age);
}
```

The generated SQL SELECT statement is shown here:

```
SELECT
...
FROM author author0_
LEFT OUTER JOIN book books1_
ON author0_.id = books1_.author_id
WHERE author0_.age < ?
ORDER BY author0_.name DESC
```

### Using Specification

Using Specification is also supported. For example, let’s assume the following classical Specification for generating WHERE age > 45:

```
public class AuthorSpecs {
private static final int AGE = 45;
public static Specificationauthor isAgeGt45() {
return (Rootauthor root,
CriteriaQuery? query, CriteriaBuilder builder)
-> builder.greaterThan(root.get("age"), AGE);
}
}
```

Let’s use this Specification:

```
@Repository
@Transactional(readOnly = true)
public interface AuthorRepository extends JpaRepositoryauthor,,
JpaSpecificationExecutorauthor {
@Override
@EntityGraph(value = "author-books-graph",
type = EntityGraph.EntityGraphType.FETCH)
public Listauthor findAll(Specification spec);
}
```

```
List authors = authorRepository.findAll(isAgeGt45());
```

The generated SQL SELECT statement is as follows:

```
SELECT
...
FROM author author0_
LEFT OUTER JOIN book books1_
ON author0_.id = books1_.author_id
WHERE author0_.age > 45
```

### Using @Query and JPQL

Pay attention to queries that are used with entity graphs that specify join fetching. In such cases, it’s mandatory to have the owner of the fetched association present in the SELECT list.

@EntityGraph(value = "author-books-graph",
type = EntityGraph.EntityGraphType.FETCH)
@Query(value="SELECT a FROM Author a WHERE a.age > 20 AND a.age < 40")
public Listauthor fetchAllAgeBetween20And40();
The SQL SELECT statement is as follows:

```
SELECT
...
FROM author author0_
LEFT OUTER JOIN book books1_
ON author0_.id = books1_.author_id
WHERE author0_.age > 20 AND author0_.age < 40
```

### consider

**MultipleBagFetchException:**

* This exception occurs when you attempt multiple eager fetches in Hibernate, and it can happen both with entity graphs and when manually constructing queries.
* The multiple eager fetches might result in loading a large number of records due to the Cartesian product, which can be a performance bottleneck.

@EntityGraph(graphType = EntityGraphType.FETCH, attributePaths = { "authors", "chapters.sections.pages" })
@Query("SELECT b FROM Book b")
List<Book> findAllWithEagerLoading();

also

```
@Entity
public class Order {
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;@OneToMany(mappedBy = "order", fetch = FetchType.EAGER)
private Setproduct products;

// Other Order fields and methods
}@Entity
public class Product {
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;@ManyToOne
@JoinColumn(name = "order_id")
private Order order;

// Other Product fields and methods
}
```

Here, if you retrieve orders with their products using eager fetching (by default, Hibernate uses Sets for one-to-many associations), you might encounter a MultipleBagFetchException.

**Solutions**

**Fetch Associations One at a Time:**

To avoid the MultipleBagFetchException and improve performance, you can fetch associations one at a time. Here's an example of how you can achieve this using separate queries:

```
@Repository
public class BookRepository {@PersistenceContext
private EntityManager entityManager;

public Listbook findAllBooks() {
    TypedQuerybook query = entityManager.createQuery("SELECT b FROM Book b", Book.class);
    return query.getResultList();
}

public Listauthor findAuthorsForBook(Book book) {
    // Fetch authors for a specific book
    return book.getAuthors();
}

public Listchapter findChaptersForBook(Book book) {
    // Fetch chapters for a specific book
    return book.getChapters();
}

// Define similar methods for fetching sections and pages.
}
```

In this solution, you fetch the list of books separately and then fetch authors, chapters, sections, and pages for each book as needed. This avoids eager loading of all associations at once and minimizes the Cartesian product.

Switching from Set to List:

To address the MultipleBagFetchException, you can switch from using `Set` to `List` for your associations. Modify the `Order` entity like this:

```
@Entity
public class Order {
@Id
@GeneratedValue(strategy = GenerationType.IDENTITY)
private Long id;@OneToMany(mappedBy = "order", fetch = FetchType.EAGER)
private Listproduct products;

// Other Order fields and methods
}
```

While this change avoids the MultipleBagFetchException, it can still be inefficient in terms of performance because it may lead to a large number of rows in the result set (cartesian product).

**Fetching One Association at a Time:**

Rather than eagerly fetching both Books and Chapters simultaneously, you can fetch one association at a time to avoid the Cartesian product issue.

```
//fetch books
EntityGraphauthor graph = entityManager.createEntityGraph(Author.class);
graph.addSubgraph("books");
Author author = entityManager.find(Author.class, authorId, Collections.singletonMap("javax.persistence.fetchgraph", graph));
// Fetch Chapters separately
EntityGraphauthor chapterGraph = entityManager.createEntityGraph(Author.class);
chapterGraph.addSubgraph("chapters");
author = entityManager.find(Author.class, authorId, Collections.singletonMap("javax.persistence.fetchgraph", chapterGraph));
```

**Native Queries with Entity Graphs:**

you cannot use entity graphs with native SQL queries in Hibernate. Entity graphs are a feature of JPA (Java Persistence API) and are not applicable to native SQL queries.

**Pagination and In-Memory Processing:**

When using entity graphs that translate into SQL JOINs for fetching associated collections, pagination may not work as expected.

In such cases, pagination could occur in-memory, leading to potential performance issues.

### Ad Hoc Entity Graphs

An ad hoc entity graph can be defined via the attributePaths element of the @EntityGraph annotation. The entity’s related associations and basic fields that should be loaded in a single SELECT are specified as a list separated by comma of type,

```
@EntityGraph(attributePaths = {"attr1", "attr2", ...}.
```

Obviously, this time, there is no need to use @NamedEntityGraph.

For example, the entity graph from the previous section can be written as follows:

```
@Repository
@Transactional(readOnly = true)
public interface AuthorRepository extends JpaRepositoryauthor, {
@Override
@EntityGraph(attributePaths = {"books"},
type = EntityGraph.EntityGraphType.FETCH)
public Listauthor findAll();
}
```

Calling findAll() triggers the same SQL SELECT statement as @NamedEntityGraph:

```
SELECT
author0_.id AS id1_0_0_,
books1_.id AS id1_1_1_,
author0_.age AS age2_0_0_,
author0_.genre AS genre3_0_0_,
author0_.name AS name4_0_0_,
books1_.author_id AS author_i4_1_1_,
books1_.isbn AS isbn2_1_1_,
books1_.title AS title3_1_1_,
books1_.author_id AS author_i4_1_0__,
books1_.id AS id1_1_0__
FROM author author0_
LEFT OUTER JOIN book books1_
ON author0_.id = books1_.author_id
```


Ad hoc entity graphs are a convenient way to keep the entity graph definition at the repository-level and not alter the entities with @NamedEntityGraph.


### Defining an Entity Graph via EntityManager

To get an entity graph directly via EntityManager, you call the getEntityGraph(String entityGraphName) method. Next, pass the return of this method to the overloaded find() method, as in the following snippet of code:

```
EntityGraph entityGraph = entityManager
.getEntityGraph("author-books-graph");
Mapstring, properties = new HashMap<>();
properties.put("javax.persistence.fetchgraph", entityGraph);
Author author = entityManager.find(Author.class, id, properties);
```

JPQL and EntityManager can be used as well:

```
EntityGraph entityGraph = entityManager
.getEntityGraph("author-books-graph");
Author author = entityManager.createQuery(
"SELECT a FROM Author a WHERE a.id = :id", Author.class)
.setParameter("id", id)
.setHint("javax.persistence.fetchgraph", entityGraph)
.getSingleResult();
Or via CriteriaBuilder and EntityManager:
EntityGraph entityGraph = entityManager
.getEntityGraph("author-books-graph");
CriteriaBuilder criteriaBuilder = entityManager.getCriteriaBuilder();
CriteriaQueryauthor criteriaQuery
= criteriaBuilder.createQuery(Author.class);
Rootauthor author = criteriaQuery.from(Author.class);
criteriaQuery.where(criteriaBuilder.equal(root.longget("id"), id));
TypedQueryauthor typedQuery = entityManager.createQuery(criteriaQuery);
typedQuery.setHint("javax.persistence.loadgraph", entityGraph);
Author author = typedQuery.getSingleResult();
```


You can create an entity graph via the EntityManager#createEntityGraph() method.
